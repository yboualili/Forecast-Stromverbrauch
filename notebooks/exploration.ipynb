{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(\n",
    "    path=\"data/raw/2019_data_15min.hdf5\", levels=[\"NO_PV\", None, \"HOUSEHOLD\"]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Converts hdf5 file to dataframe and perform preliminary processing\n",
    "\n",
    "    Args:\n",
    "        path (str, optional): path to file. Defaults to 'data/raw/2019_data_15min.hdf5'.\n",
    "        levels (list, optional): levels in tree. Defaults to ['NO_PV', None,'HOUSEHOLD'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: master dataframe\n",
    "    \"\"\"\n",
    "    households = []\n",
    "    df_all = []\n",
    "\n",
    "    f = h5py.File(path, \"r\")\n",
    "\n",
    "    if levels[1] is None:\n",
    "        # acccess all households\n",
    "        households = list(f[levels[0]].keys())\n",
    "    else:\n",
    "        households = levels[1]\n",
    "\n",
    "    for household in households:\n",
    "        df_household = pd.DataFrame(f[levels[0]][household][levels[2]][\"table\"][:])\n",
    "        df_household[\"index\"] = pd.to_datetime(\n",
    "            df_household[\"index\"], unit=\"s\", utc=True\n",
    "        )\n",
    "        df_household.set_index(\"index\", inplace=True)\n",
    "        df_household = df_household.add_prefix(f\"{household}_\")\n",
    "        df_all.append(df_household)\n",
    "    return pd.concat(df_all, axis=1)\n",
    "\n",
    "\n",
    "df_2018 = load_df(\"../data/raw/2018_data_15min.hdf5\")\n",
    "df_2019 = load_df(\"../data/raw/2019_data_15min.hdf5\")\n",
    "df_2020 = load_df(\"../data/raw/2020_data_15min.hdf5\")\n",
    "\n",
    "\n",
    "data = pd.concat([df_2018, df_2019, df_2020], axis=0)\n",
    "print(f\"data.shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum all households\n",
    "s_tot_households = list(filter(lambda x: x.endswith(\"_S_TOT\"), data.columns))\n",
    "data[\"TARGET\"] = data[s_tot_households].sum(axis=1).shift(periods=-4 * 24)\n",
    "\n",
    "# drop nans in target\n",
    "data = data[data[\"TARGET\"].notna()]\n",
    "data.index.names = [\"date\"]\n",
    "data.to_pickle(\"../data/preprocessed/master_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% % script false - -no-raise-error\n",
    "profile = ProfileReport(data, correlations=None,\n",
    "                        dark_mode=True, minimal=True)\n",
    "profile.to_file(\"../docs/raw_pandas_profiling.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.sample(n=1000, replace=False, axis=0, random_state=1).corr()\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_corr_features = corr[\"TARGET\"].abs().sort_values(ascending=False)\n",
    "highest_corr_featurenames = highest_corr_features.index.to_list()\n",
    "highest_corr_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the six most correlated features\n",
    "\n",
    "# create random sample with nobservations\n",
    "sample = data.sample(n=1000, replace=False, axis=0, random_state=1)\n",
    "\n",
    "# Highest correlation\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        sns.scatterplot(\n",
    "            x=sample[highest_corr_featurenames[i * 3 + j]],\n",
    "            y=sample[\"TARGET\"],\n",
    "            ax=ax[i, j],\n",
    "        )\n",
    "\n",
    "plt.suptitle(\"Distribution of most correlated features with target on sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze distributions of observations over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data.groupby(level=\"date\")[\"TARGET\"].agg([\"count\"])\n",
    "counts.columns = counts.columns.to_flat_index()\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "sns.distplot(x=counts[\"count\"], ax=ax[0])\n",
    "sns.lineplot(data=counts, x=\"date\", y=\"count\", ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze distribution of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# create plot with histogram and distribution\n",
    "sns.distplot(data[\"TARGET\"])\n",
    "\n",
    "print(f\"Mean of target:{data['TARGET'].mean()}\")\n",
    "print(f\"Minimum value of target:{data['TARGET'].min()}\")\n",
    "print(f\"Maximum value of target:{data['TARGET'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze distribution of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\"SFH10_S_1\", \"SFH10_S_2\", \"SFH10_S_3\", \"SFH10_S_TOT\"]\n",
    "\n",
    "# create random sample with n observations\n",
    "sample = data.sample(n=1000, replace=False, axis=0, random_state=1)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        sns.violinplot(y=sample[selected_features[i * 2 + j]], ax=ax[i, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adopted from here: https://www.kaggle.com/prashant111/comprehensive-guide-on-feature-selection?scriptVersionId=47174422&cellId=30\n",
    "sel = VarianceThreshold(threshold=0.05)\n",
    "sel.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([x for x in data.columns if x not in data.columns[sel.get_support()]]))\n",
    "\n",
    "[x for x in data.columns if x not in data.columns[sel.get_support()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze mean and standard deviation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats = data.groupby(by=[data.index.year, data.index.month]).agg([\"mean\", \"std\"])\n",
    "data_stats.columns = data_stats.columns.to_flat_index()\n",
    "data_stats.columns = [\"_\".join(tups) for tups in data_stats.columns]\n",
    "data_stats.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\"TARGET_mean\", \"TARGET_std\"]\n",
    "data_stats_selected = data_stats[selected_features]\n",
    "data_stats_selected.reset_index(inplace=True, drop=True)\n",
    "\n",
    "sns.lineplot(data=data_stats_selected)\n",
    "plt.suptitle(\"Load over time\")\n",
    "plt.ylabel(\"Load\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4d0325e23e769b411b587a85455e4fc45b5fc1944d80bf8e902541644961e2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
